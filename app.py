import os
import streamlit as st
import torch
import pickle
import glob
from typing import List
from langchain_community.document_loaders import PDFPlumberLoader
from langchain.schema import Document
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_community.vectorstores import FAISS
from langchain_community.llms import Ollama
from elasticsearch import Elasticsearch
from elasticsearch.helpers import bulk
import numpy as np
import faiss
import traceback
from langchain.docstore import InMemoryDocstore
from langchain_community.chat_models import ChatTongyi
from pathlib import Path

#å½“æ—¶æ˜¯ç”¨äºè§£å†³stramlitçš„æŸä¸ªå†²çªé—®é¢˜
torch.classes.__path__ = [os.path.join(torch.__path__[0], torch.classes.__file__)] 

# ======== æç¤ºè¯ç»Ÿä¸€ç®¡ç†ï¼ˆç”¨SystemMessageå’ŒHumanMessageï¼‰ ========
PROMPTS = {
    "is_scientific_query": [
        SystemMessage(content="è¯·åˆ¤æ–­ä»¥ä¸‹é—®é¢˜æ˜¯å¦å±äºâ€œç§‘æ™®ç›¸å…³å†…å®¹â€ï¼ˆå¦‚ç§‘å­¦ã€æŠ€æœ¯ã€æ¼”è®²ç­‰ï¼‰ï¼Œè¯·åŠ¡å¿…åªå›ç­”â€œæ˜¯â€æˆ–â€œå¦â€ï¼š"),
        HumanMessage(content="{question}")
    ],
    "rag_answer": [
        SystemMessage(content="ä½ æ˜¯ç†æ€§ä¸”å‹å¥½çš„ç§‘å­¦é—®ç­”åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹ç§‘æ™®å†…å®¹ï¼Œåšä¸ªå…¨é¢çš„æ€»ç»“å’Œæ€è€ƒï¼Œå›ç­”ç”¨æˆ·é—®é¢˜ã€‚å¦‚æœæ‰¾ä¸åˆ°ç­”æ¡ˆï¼Œè¯·å›å¤â€œæŠ±æ­‰å“¦ï¼Œæˆ‘çš„çŸ¥è¯†åº“é‡Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„å†…å®¹ï¼â€ã€‚"),
        HumanMessage(content="ä¸Šä¸‹æ–‡ï¼š\n{context}\n\né—®é¢˜ï¼š\n{question}")
    ],
    "simple_answer": [
        SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªæ´»æ³¼å¯çˆ±åˆç†æ€§çš„ç§‘æ™®é—®ç­”å°åŠ©æ‰‹ï¼Œè¯·ç®€æ´ã€å‹å¥½ã€ç¤¼è²Œåœ°å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š"),
        HumanMessage(content="{question}")
    ],
    #æ·»åŠ ä¸€ä¸ªæ¨¡æ¿ï¼Œç”¨äºå¤„ç†å¤šè½®é—®ç­”
    "multi_turn": [
        SystemMessage(content="ä½ æ˜¯ç†æ€§ä¸”å‹å¥½çš„ç§‘å­¦é—®ç­”åŠ©æ‰‹ã€‚è¯·æ ¹æ®å¯¹è¯å†å²å’Œä»¥ä¸‹ç§‘æ™®å†…å®¹å›ç­”é—®é¢˜ï¼š\n{context}"),
        HumanMessage(content="å½“å‰å¯¹è¯å†å²ï¼š\n{history}\n\né—®é¢˜ï¼š\n{question}")
    ],
}


# # é…ç½®è·¯å¾„å’Œç¯å¢ƒå˜é‡
# DATA_DIR = "/rag_science_speech/data_pdf/merged_pdfs"
# VECTOR_STORE_PATH = "/rag_science_speech/vector_store.pkl"
# ES_INDEX_NAME = "rag_docs"
# os.environ['HF_ENDPOINT'] = "https://hf-mirror.com"


# #é…ç½®è·¯å¾„å’Œç¯å¢ƒå˜é‡ï¼Œè·¯å¾„éœ€è¦ä¿®æ”¹
# DATA_DIR = "/root/autodl-tmp/jinxiangshao_projects1/rag_science_speech/data_pdf/merged_pdfs"
# VECTOR_STORE_PATH = "/root/autodl-tmp/jinxiangshao_projects1/rag_science_speech/vector_store.pkl"
# ES_INDEX_NAME = "rag_docs"
# os.environ['HF_ENDPOINT'] = "https://hf-mirror.com"


# # ==== åŠ¨æ€è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆå…³é”®ä¿®æ”¹ï¼‰====
# BASE_DIR = Path(__file__).parent  # è·å–å½“å‰æ–‡ä»¶ï¼ˆapp.pyï¼‰æ‰€åœ¨çš„ç›®å½•ï¼Œå³é¡¹ç›®æ ¹ç›®å½•

# # ==== ä¿®æ”¹ DATA_DIR å’Œ VECTOR_STORE_PATH ä¸ºç›¸å¯¹è·¯å¾„ ====
# # æ•°æ®ç›®å½•ï¼šç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•çš„ "data_pdf/merged_pdfs"
# DATA_DIR = BASE_DIR / "data_pdf/merged_pdfs"
# # å‘é‡å­˜å‚¨è·¯å¾„ï¼šç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•çš„ "vector_store/vector_store.pkl"
# VECTOR_STORE_PATH = BASE_DIR / "vector_store/vector_store.pkl"

# # å°† Path å¯¹è±¡è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼ˆä¾›åç»­ä»£ç ä½¿ç”¨ï¼‰
# DATA_DIR = str(DATA_DIR)
# VECTOR_STORE_PATH = str(VECTOR_STORE_PATH)

# ç›´æ¥ä»å½“å‰å·¥ä½œç›®å½•å‡ºå‘æ‰¾å­ç›®å½•/æ–‡ä»¶
DATA_DIR = "data_pdf/merged_pdfs"  
VECTOR_STORE_PATH = "vector_store.pkl"  

ES_INDEX_NAME = "rag_docs"
os.environ['HF_ENDPOINT'] = "https://hf-mirror.com"



#è·å–å‘é‡æ¨¡å‹
@st.cache_resource
def get_embeddings():
    try:
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        model_kwargs = {'device': device}
        embedder = HuggingFaceEmbeddings(
            model_name="lier007/xiaobu-embedding-v2",
            model_kwargs=model_kwargs,
            encode_kwargs={'normalize_embeddings': True}
        )
        # æµ‹è¯•åµŒå…¥åŠŸèƒ½æ˜¯å¦æ­£å¸¸
        _ = embedder.embed_query("æµ‹è¯•")
        return embedder
    except Exception:
        return None

#è®¾ç½®æˆè‡ªå·±çš„å¯†é’¥
os.environ["TONGYI_API_KEY"] = "sk-506a5c243ac3445caea6be389b0025fb"  



# è¾…åŠ©å‡½æ•°ï¼šæ ¼å¼åŒ–æ¶ˆæ¯ä¸ºé€šä¹‰æ‰€éœ€æ ¼å¼ï¼Œä¸ä¹‹å‰æœ‰ä¸åŒ
def format_messages(template_messages, **kwargs):
    messages = []
    for msg in template_messages:
        content = msg.content.format(**kwargs)
        if isinstance(msg, SystemMessage):
            messages.append({"role": "system", "content": content})
        else:
            messages.append({"role": "user", "content": content})
    return messages




# è·å–é€šä¹‰åƒé—® LLM å®¢æˆ·ç«¯
def get_llm():
    api_key = os.getenv("TONGYI_API_KEY")
    if not api_key:
        st.error("é€šä¹‰APIå¯†é’¥æœªé…ç½®ï¼")
        return None
    try:
        return ChatTongyi(
            model_name="qwen-plus",
            dashscope_api_key=api_key,
            temperature=0.7
        )
    except Exception as e:
        st.error(f"é€šä¹‰æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        return None



def load_documents() -> List[Document]:
    docs = []
    for file_path in glob.glob(f"{DATA_DIR}/*.pdf"):
        try:
            loader = PDFPlumberLoader(file_path)
            pages = loader.load()
            full_text = "\n".join([p.page_content for p in pages])
            file_name = os.path.basename(file_path)
            docs.append(Document(page_content=full_text, metadata={"source": file_name}))
        except Exception:
            continue
    return docs





# å®šä¹‰ä¸€ä¸ªç®€å•å‘é‡æ•°æ®åº“æ›¿ä»£faissï¼ˆå¦‚æœfaisså¤±æ•ˆçš„è¯ï¼‰
# å¯ä»¥ä¸ç”¨è¿™éƒ¨åˆ†

class InMemorySimpleVectorStore:
    def __init__(self, embedding_function, texts, metadatas=None):
        self.embedding_function = embedding_function
        self.texts = texts
        self.metadatas = metadatas or [{} for _ in texts]
        self.doc_embeddings = []
        for text in texts:
            try:
                embedding = embedding_function.embed_query(text)
                self.doc_embeddings.append(embedding)
            except Exception:
                self.doc_embeddings.append(np.zeros(768, dtype=np.float32))

    def similarity_search(self, query, k=4):
        query_embedding = self.embedding_function.embed_query(query)
        query_vec = np.array(query_embedding, dtype=np.float32)
        similarities = []
        for doc_embedding in self.doc_embeddings:
            doc_vec = np.array(doc_embedding, dtype=np.float32)
            dot_product = np.dot(query_vec, doc_vec)
            query_norm = np.linalg.norm(query_vec)
            doc_norm = np.linalg.norm(doc_vec)
            similarity = dot_product / (query_norm * doc_norm) if query_norm > 0 and doc_norm > 0 else 0
            similarities.append(similarity)
        top_indices = np.argsort(similarities)[-k:][::-1]
        return [Document(page_content=self.texts[i], metadata=self.metadatas[i]) for i in top_indices]




#åŠ è½½å‘é‡æ•°æ®åº“çš„å‡½æ•°
def load_or_create_vector_store():
    if os.path.exists(VECTOR_STORE_PATH):
        try:
            with open(VECTOR_STORE_PATH, "rb") as f:
                vector_store = pickle.load(f)
            return vector_store
        except Exception:
            return None
    return None


#æ„å»ºå‘é‡æ•°æ®åº“å’Œç´¢å¼•
@st.cache_resource
def build_vector_store_and_index():
    vector_store = load_or_create_vector_store()
    if vector_store:
        try:
            es = Elasticsearch("http://localhost:9200")
            es.info()
            return vector_store, es
        except Exception:
            return vector_store, None

    docs = load_documents()
    embedder = get_embeddings()
    if not embedder:
        return None, None

    docs = [doc for doc in docs if doc.page_content and doc.page_content.strip()]
    if not docs:
        return None, None

    try:
        try:
            vector_store = FAISS.from_documents(docs, embedder)
        except Exception:
            texts = [doc.page_content for doc in docs]
            metadatas = [doc.metadata for doc in docs]
            vector_store = InMemorySimpleVectorStore(embedder, texts, metadatas)

        os.makedirs(os.path.dirname(VECTOR_STORE_PATH), exist_ok=True)
        with open(VECTOR_STORE_PATH, "wb") as f:
            pickle.dump(vector_store, f)

        try:
            es = Elasticsearch("http://localhost:9200")
            es.info()
            if not es.indices.exists(index=ES_INDEX_NAME):
                actions = [{
                    "_index": ES_INDEX_NAME,
                    "_id": i,
                    "_source": {
                        "content": doc.page_content,
                        "source": doc.metadata.get("source", "unknown")
                    }
                } for i, doc in enumerate(docs)]
                bulk(es, actions)
            return vector_store, es
        except Exception:
            return vector_store, None

    except Exception:
        return None, None



#å…³é”®å­—åŒ¹é…        
def keyword_search(es, query, top_k=5):
    if not es:
        return []
    try:
        res = es.search(
            index=ES_INDEX_NAME,
            query={"match": {"content": query}},
            size=top_k
        )
        return [Document(page_content=hit["_source"]["content"], metadata={"source": hit["_source"]["source"]}) 
                for hit in res["hits"]["hits"]]
    except Exception:
        return []


# åˆ¤æ–­é—®é¢˜æ˜¯å¦ä¸ºç§‘æ™®é—®é¢˜ è¿”å›Trueæˆ–False
def is_scientific_query(question: str) -> bool:
    llm = get_llm()
    if not llm:
        return True
    try:
        messages = format_messages(PROMPTS["is_scientific_query"], question=question)
        #è¿™é‡Œå’ŒåŸå…ˆä¸ä¸€æ ·æ˜¯å› ä¸ºå¦‚æœç›´æ¥ä½¿ç”¨åŸæ¥çš„ä»£ç ï¼Œä¼šé¢å¤–è¾“å‡ºä¸€äº›å…¶ä»–å†…å®¹ï¼Œè¿™é‡Œæ˜¯ä¸ºäº†è°ƒæ•´è¾“å‡ºæ ¼å¼ï¼Œåé¢ç±»ä¼¼çš„éƒ¨åˆ†ä¹Ÿè¦ä¿®æ”¹
        response = llm.invoke(messages)
        # æå–çº¯æ–‡æœ¬å†…å®¹
        if hasattr(response, 'content'):
            result = response.content
        else:
            result = str(response)
        result = result.strip().lower()
        return result.startswith("æ˜¯") or "æ˜¯" in result or result.startswith("yes")
    except Exception:
        return True


#ä¿®æ”¹å›ç­”ç”Ÿæˆéƒ¨åˆ†ï¼Œæ ¹æ®æ˜¯å¦æ˜¯å¤šè½®å¯¹è¯åŠ¨æ€æ„å»ºæç¤ºï¼ŒæŠŠmainå‡½æ•°é‡Œé¢çš„å›ç­”ç”Ÿæˆéƒ¨åˆ†å°è£…æˆä¸€ä¸ªå‡½æ•°ï¼Œç›´æ¥è°ƒç”¨
def generate_response(user_input, is_sci_question, rag_docs=None):
    llm = get_llm()
    if not llm:
        return "ç³»ç»Ÿé”™è¯¯ï¼šæ— æ³•åŠ è½½è¯­è¨€æ¨¡å‹ã€‚"
    
    # æ„å»ºå¯¹è¯å†å²å­—ç¬¦ä¸²
    history_str = "\n".join(
        [f"{msg['role']}: {msg['content']}" 
         for msg in st.session_state.chat_history]
    )
    
    if is_sci_question and rag_docs:
        # æ„å»ºRAGä¸Šä¸‹æ–‡
        context = "\n".join(
            [f"æ–‡æ¡£({i+1}): {doc.page_content}" 
             for i, doc in enumerate(rag_docs)]
        )
        
        # ä½¿ç”¨å¤šè½®å¯¹è¯æ¨¡æ¿
        messages = format_messages(
            PROMPTS["multi_turn"],
            context=context,
            history=history_str,
            question=user_input
        )
    else:
        # æ™®é€šå¯¹è¯ä½¿ç”¨ç®€å•æ¨¡æ¿
        messages = format_messages(
            PROMPTS["simple_answer"],
            question=user_input
        )
        # æ·»åŠ å†å²ä¸Šä¸‹æ–‡
        messages.insert(0, {"role": "system", "content": f"å¯¹è¯å†å²ï¼š\n{history_str}"})
    
    try:
        response = llm.invoke(messages)
        return response.content.strip() if hasattr(response, 'content') else str(response).strip()
    except Exception as e:
        return f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™ï¼š{str(e)}"


def main():

    st.set_page_config(page_title="ç§‘æ™®çŸ¥è¯†é—®ç­”åŠ©æ‰‹", layout="wide")
    st.title("ğŸŒŸ ç§‘æ™®çŸ¥è¯†é—®ç­”åŠ©æ‰‹")

    vector_store, es = build_vector_store_and_index()
    if vector_store is None:
        st.error("å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥ï¼Œç³»ç»Ÿå¯èƒ½æ— æ³•æ­£å¸¸å›ç­”ç§‘æ™®ç›¸å…³é—®é¢˜ã€‚")
    if es is None:
        st.warning("Elasticsearchè¿æ¥å¤±è´¥ï¼Œå°†ä»…ä½¿ç”¨å‘é‡æœç´¢ã€‚")

    #ä¿®æ”¹åˆå§‹åŒ–
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = [
        {"role": "system", "content": "ä½ æ˜¯ç†æ€§ä¸”å‹å¥½çš„ç§‘å­¦é—®ç­”åŠ©æ‰‹"}
                 ]

    # æ˜¾ç¤ºå†å²æ¶ˆæ¯ï¼ˆè·³è¿‡ç³»ç»Ÿæ¶ˆæ¯ï¼‰
    for msg in st.session_state.chat_history:
        if msg["role"] == "system":  # è·³è¿‡ç³»ç»Ÿæ¶ˆæ¯
            continue
        with st.chat_message(msg["role"]):
            st.markdown(msg["content"])

    user_input = st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜~")

    if user_input:
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²
        user_message = {"role": "user", "content": user_input}
        st.session_state.chat_history.append(user_message)
        
        with st.chat_message("User"):
            st.markdown(user_input)

        with st.spinner("æ€è€ƒä¸­..."):
            try:
                is_sci_question = is_scientific_query(user_input)
                rag_docs = []
                if is_sci_question and vector_store:
                    

                    try:
                        vector_results = vector_store.similarity_search(user_input, k=5)
                        print("vector_results:",vector_results)
                        rag_docs.extend(vector_results)
                        
                    except Exception:
                        pass

                    if es:
                        try:
                            keyword_results = keyword_search(es, user_input, top_k=5)
                            print("keyword_results:", keyword_results)
                            rag_docs.extend(keyword_results)
                        except Exception:
                            pass
                     # å»é‡å’Œæ’åº
                    if rag_docs:
                        seen_content = set()
                        unique_docs = []
                        for doc in rag_docs:
                            if doc.page_content not in seen_content:
                                seen_content.add(doc.page_content)
                                unique_docs.append(doc)
                        rag_docs = sorted(unique_docs, key=lambda x: len(x.page_content))[:5]
                answer = generate_response(
                                        user_input, 
                                         is_sci_question, 
                                     rag_docs if rag_docs else None  # å¦‚æœrag_docsä¸ä¸ºç©ºåˆ™ä¼ å…¥ï¼Œå¦åˆ™ä¼ å…¥None
                                            )
                    # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å†å²
                assistant_message = {"role": "assistant", "content": answer}
                st.session_state.chat_history.append(assistant_message)
                    # æ˜¾ç¤ºåŠ©æ‰‹å›å¤
                with st.chat_message("assistant"):  # è§’è‰²åç§°å°å†™
                    st.markdown(answer)

            except Exception as e:
                # é”™è¯¯å¤„ç†
                error_msg = f"ç³»ç»Ÿé”™è¯¯ï¼š{str(e)}"
                st.error(error_msg)
                st.error(traceback.format_exc())
                
                # æ·»åŠ é”™è¯¯æ¶ˆæ¯åˆ°å†å²
                assistant_message = {"role": "assistant", "content": error_msg}
                st.session_state.chat_history.append(assistant_message)
                
                # æ˜¾ç¤ºé”™è¯¯æ¶ˆæ¯
                with st.chat_message("assistant"):
                    st.markdown(error_msg)

if __name__ == "__main__":
    main()









